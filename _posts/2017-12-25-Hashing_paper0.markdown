---
layout: post
title:  "Notes on Hashing 01"					# Title of the post
modified: 2017-12-26				# Date
category: Academic
tags: []
comments: true
mathjax: true
---
{% include base.html %}
* ## Supervised Hashing for Image Retrieval via Image Representation Learning([AAAI2014](http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf "AAAI2017"))

Introducing a Two-Stage learning Scheme.

A Simlarity Matrix $$S \in \{-1,1\}^{n\times n}$$ is decomposed into $$HH^T$$ where $$H \in \{-1,1\}^{n\times q}$$ (Ofc a specific coordinate ascend updating scheme is given, with relaxation that $$H \in [-1,1]^{n\times n}$$)

![algorithm](http://p1k0vaa5f.bkt.clouddn.com/2017-12-26_AAAI_1.png){:height="60%" width="60%"}

Then learned codes $$H$$ is used to guide the learning of deep conv networks (Used [classical Conv_Net 2012](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)). Features in last FC layer are used to learn **Binary codes** and **label info** simultaneously.

![stage pic](http://p1k0vaa5f.bkt.clouddn.com/2017-12-26_AAAI_2.png)
### (最后的分支像不像GAN?)

**[Existing Study](http://www.yugangjiang.info/publication/CVPR12-KSH.pdf) showed that the code inner product $$H_{i\cdot }H^T_{j\cdot}$$ has one to one correspondence to the Hamming distance between $$H_{i\cdot }$$ and $$H_{j\cdot}$$**
$$\min_H\sum_{i=1}^{n}\sum_{j=1}^{n}(S_{ij} - \frac{1}{q}H_{i\cdot }H^T_{j\cdot})^2 = \min_H||S-HH^T||_F^2$$



* ## Simultaneous Feature Learning and Hash Coding with Deep Neural Networks([CVPR2015](http://arxiv.org/pdf/1504.03410v1.pdf "CVPR2015"))

A **triplet ranking** was introduced in modeling the similarity of the images. Image $$I$$ is more similar to image$$I^+$$ than to image $$I^{-}$$. The loss based on this is proposed as well (The trpilets of $$I, I^+, I^-$$ is selected from data with their label information): 

$$l_{triplet}(\mathcal{F}(I),\mathcal{F}(I^+),\mathcal{F}(I^-))$$ 

$$=max(0,||\mathcal{F}(I) - \mathcal{F}(I^+)||^2_2 - ||\mathcal{F}(I) - \mathcal{F}(I^-)||^2_2 + 1 )$$

$$s.t. \mathcal{F}(I), \mathcal{F}(I)^+, \mathcal{F}(I)^- \in [0,1]^q$$

The the gradients were given:

$$\frac{\partial l}{\partial b} = (2b^- - 2b^+) \times I_{||b - b^+||^2_2 - ||b-b^-||^2_2+1 >0}$$

$$\frac{\partial l}{\partial b^+} = (2b^+ - 2b) \times I_{||b - b^+||^2_2 - ||b-b^-||^2_2+1 >0}$$

$$\frac{\partial l}{\partial b^-} = (2b^- - 2b) \times I_{||b - b^+||^2_2 - ||b-b^-||^2_2+1 >0}$$

A network architecture is the divided as feature extractor and **divide-and-encode module**

![net_achi](http://p1k0vaa5f.bkt.clouddn.com/2017-12-26_net_ach.png)

At the end of extractor (avg pooling), conv1x1 is used to extract features and extend the dimension to $$50 \times q$$ bits, then we have $$50 \times q$$ outputs. All these nodes were divided into q groups, each group only produces one node. $$q$$-bits of output were collected as the hash codes.

![net_achi2](http://p1k0vaa5f.bkt.clouddn.com/2017-12-26_net_ach2.png){: height="50%" width="50%"}

[Batch-Orthogonal Locality Sensitive Hashing](http://ieeexplore.ieee.org/abstract/document/6783789/) theoretically and empirically shows that **hash codes generated by batch-orthogonalized random projections are superior to those generated by simple random projections**, where batch-orthogonalized projections generate fewer redundant hash bits than random projections